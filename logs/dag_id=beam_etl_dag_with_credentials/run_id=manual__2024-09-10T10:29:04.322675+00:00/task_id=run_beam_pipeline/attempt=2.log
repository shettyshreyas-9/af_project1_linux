[2024-09-10T16:04:28.542+0530] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-10T16:04:28.551+0530] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: beam_etl_dag_with_credentials.run_beam_pipeline manual__2024-09-10T10:29:04.322675+00:00 [queued]>
[2024-09-10T16:04:28.557+0530] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: beam_etl_dag_with_credentials.run_beam_pipeline manual__2024-09-10T10:29:04.322675+00:00 [queued]>
[2024-09-10T16:04:28.557+0530] {taskinstance.py:2865} INFO - Starting attempt 2 of 2
[2024-09-10T16:04:28.584+0530] {taskinstance.py:2888} INFO - Executing <Task(PythonOperator): run_beam_pipeline> on 2024-09-10 10:29:04.322675+00:00
[2024-09-10T16:04:28.588+0530] {standard_task_runner.py:72} INFO - Started process 196042 to run task
[2024-09-10T16:04:28.592+0530] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--job-id', '83', '--raw', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py', '--cfg-path', '/tmp/tmp74a02oh7']
[2024-09-10T16:04:28.592+0530] {standard_task_runner.py:105} INFO - Job 83: Subtask run_beam_pipeline
[2024-09-10T16:04:28.643+0530] {task_command.py:467} INFO - Running <TaskInstance: beam_etl_dag_with_credentials.run_beam_pipeline manual__2024-09-10T10:29:04.322675+00:00 [running]> on host PG0392KD.springernature.com
[2024-09-10T16:04:28.718+0530] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='beam_etl_dag_with_credentials' AIRFLOW_CTX_TASK_ID='run_beam_pipeline' AIRFLOW_CTX_EXECUTION_DATE='2024-09-10T10:29:04.322675+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-09-10T10:29:04.322675+00:00'
[2024-09-10T16:04:28.719+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-09-10T16:04:28.719+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-09-10T16:04:28.719+0530] {logging_mixin.py:190} INFO - Current task name:run_beam_pipeline state:running start_date:2024-09-10 10:34:28.552273+00:00
[2024-09-10T16:04:28.719+0530] {logging_mixin.py:190} INFO - Dag name:beam_etl_dag_with_credentials and current dag run status:running
[2024-09-10T16:04:28.719+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-10T16:04:28.776+0530] {pipeline_options.py:372} WARNING - Discarding unparseable args: ['tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--local', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py']
[2024-09-10T16:04:28.779+0530] {pipeline_options.py:372} WARNING - Discarding unparseable args: ['tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--local', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py']
[2024-09-10T16:04:29.616+0530] {pipeline_options.py:940} WARNING - Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-09-10T16:04:29.619+0530] {pipeline_options.py:372} WARNING - Discarding unparseable args: ['tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--local', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py']
[2024-09-10T16:04:29.621+0530] {pipeline_options.py:372} WARNING - Discarding unparseable args: ['tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--local', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py']
[2024-09-10T16:04:30.177+0530] {pipeline_options.py:940} WARNING - Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-09-10T16:04:31.869+0530] {pipeline_options.py:372} WARNING - Discarding unparseable args: ['tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--local', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py']
[2024-09-10T16:04:31.871+0530] {pipeline_options.py:372} WARNING - Discarding unparseable args: ['tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--local', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py']
[2024-09-10T16:04:32.406+0530] {pipeline_options.py:940} WARNING - Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-09-10T16:04:32.409+0530] {pipeline_options.py:372} WARNING - Discarding unparseable args: ['tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--local', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py']
[2024-09-10T16:04:32.411+0530] {pipeline_options.py:372} WARNING - Discarding unparseable args: ['tasks', 'run', 'beam_etl_dag_with_credentials', 'run_beam_pipeline', 'manual__2024-09-10T10:29:04.322675+00:00', '--local', '--subdir', 'DAGS_FOLDER/beam_etl_dag.py']
[2024-09-10T16:04:34.682+0530] {pipeline_options.py:940} WARNING - Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.
[2024-09-10T16:04:35.254+0530] {dataflow_runner.py:397} INFO - Pipeline has additional dependencies to be installed in SDK worker container, consider using the SDK container image pre-building workflow to avoid repetitive installations. Learn more on https://cloud.google.com/dataflow/docs/guides/using-custom-containers#prebuild
[2024-09-10T16:04:35.255+0530] {environments.py:314} INFO - Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/beam_python3.10_sdk:2.58.1
[2024-09-10T16:04:35.255+0530] {environments.py:321} INFO - Python SDK container image set to "gcr.io/cloud-dataflow/v1beta3/beam_python3.10_sdk:2.58.1" for Docker environment
[2024-09-10T16:04:35.376+0530] {apiclient.py:667} INFO - Starting GCS upload to gs://sn_insights_test/staging/etl-pipeline-job.1725964475.371996/submission_environment_dependencies.txt...
[2024-09-10T16:04:39.917+0530] {apiclient.py:677} INFO - Completed GCS upload to gs://sn_insights_test/staging/etl-pipeline-job.1725964475.371996/submission_environment_dependencies.txt in 4 seconds.
[2024-09-10T16:04:39.919+0530] {apiclient.py:667} INFO - Starting GCS upload to gs://sn_insights_test/staging/etl-pipeline-job.1725964475.371996/pipeline.pb...
[2024-09-10T16:04:45.164+0530] {apiclient.py:677} INFO - Completed GCS upload to gs://sn_insights_test/staging/etl-pipeline-job.1725964475.371996/pipeline.pb in 5 seconds.
[2024-09-10T16:04:48.025+0530] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/dags/beam_etl_dag.py", line 32, in run_beam_pipeline
    with beam.Pipeline(options=options) as p:
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/pipeline.py", line 613, in __exit__
    self.result = self.run()
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/pipeline.py", line 563, in run
    self._options).run(False)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/pipeline.py", line 587, in run
    return self.runner.run_pipeline(self, self._options)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/runners/dataflow/dataflow_runner.py", line 502, in run_pipeline
    self.dataflow_client.create_job(self.job), self)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/utils/retry.py", line 298, in wrapper
    return fun(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/runners/dataflow/internal/apiclient.py", line 725, in create_job
    return self.submit_job_description(job)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/utils/retry.py", line 298, in wrapper
    return fun(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/runners/dataflow/internal/apiclient.py", line 843, in submit_job_description
    raise DataflowJobAlreadyExistsError(
apache_beam.runners.dataflow.internal.apiclient.DataflowJobAlreadyExistsError: There is already active job named etl-pipeline-job with id: 2024-09-10_03_33_51-17949340958295335037. If you want to submit a second job, try again by setting a different name using --job_name.
[2024-09-10T16:04:48.029+0530] {logging_mixin.py:190} INFO - Task instance in failure state
[2024-09-10T16:04:48.029+0530] {logging_mixin.py:190} INFO - Task start:2024-09-10 10:34:28.552273+00:00 end:2024-09-10 10:34:48.028817+00:00 duration:19.476544
[2024-09-10T16:04:48.029+0530] {logging_mixin.py:190} INFO - Task:<Task(PythonOperator): run_beam_pipeline> dag:<DAG: beam_etl_dag_with_credentials> dagrun:<DagRun beam_etl_dag_with_credentials @ 2024-09-10 10:29:04.322675+00:00: manual__2024-09-10T10:29:04.322675+00:00, state:running, queued_at: 2024-09-10 10:29:04.381053+00:00. externally triggered: True>
[2024-09-10T16:04:48.029+0530] {logging_mixin.py:190} INFO - Failure caused by There is already active job named etl-pipeline-job with id: 2024-09-10_03_33_51-17949340958295335037. If you want to submit a second job, try again by setting a different name using --job_name.
[2024-09-10T16:04:48.029+0530] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=beam_etl_dag_with_credentials, task_id=run_beam_pipeline, run_id=manual__2024-09-10T10:29:04.322675+00:00, execution_date=20240910T102904, start_date=20240910T103428, end_date=20240910T103448
[2024-09-10T16:04:48.057+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-10T16:04:48.057+0530] {standard_task_runner.py:124} ERROR - Failed to execute job 83 for task run_beam_pipeline (There is already active job named etl-pipeline-job with id: 2024-09-10_03_33_51-17949340958295335037. If you want to submit a second job, try again by setting a different name using --job_name.; 196042)
Traceback (most recent call last):
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/dags/beam_etl_dag.py", line 32, in run_beam_pipeline
    with beam.Pipeline(options=options) as p:
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/pipeline.py", line 613, in __exit__
    self.result = self.run()
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/pipeline.py", line 563, in run
    self._options).run(False)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/pipeline.py", line 587, in run
    return self.runner.run_pipeline(self, self._options)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/runners/dataflow/dataflow_runner.py", line 502, in run_pipeline
    self.dataflow_client.create_job(self.job), self)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/utils/retry.py", line 298, in wrapper
    return fun(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/runners/dataflow/internal/apiclient.py", line 725, in create_job
    return self.submit_job_description(job)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/utils/retry.py", line 298, in wrapper
    return fun(*args, **kwargs)
  File "/home/sst7260/airflow_projects/project1/env/lib/python3.10/site-packages/apache_beam/runners/dataflow/internal/apiclient.py", line 843, in submit_job_description
    raise DataflowJobAlreadyExistsError(
apache_beam.runners.dataflow.internal.apiclient.DataflowJobAlreadyExistsError: There is already active job named etl-pipeline-job with id: 2024-09-10_03_33_51-17949340958295335037. If you want to submit a second job, try again by setting a different name using --job_name.
[2024-09-10T16:04:48.096+0530] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-10T16:04:48.103+0530] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-10T16:04:48.103+0530] {local_task_job_runner.py:245} INFO - ::endgroup::
